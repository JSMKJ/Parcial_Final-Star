{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto de Clasificaci√≥n Multiclase: Clasificaci√≥n Estelar\n",
    "\n",
    "**Curso:** Inteligencia Artificial  \n",
    "**Tema:** Clasificaci√≥n Multiclase con Alta Dimensionalidad  \n",
    "**Dataset:** Stellar Classification Dataset - SDSS17\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivo del Proyecto\n",
    "\n",
    "Este proyecto tiene como objetivo desarrollar un modelo de clasificaci√≥n multiclase para identificar objetos astron√≥micos (estrellas, galaxias y cu√°sares) utilizando datos del Sloan Digital Sky Survey (SDSS). El proyecto incluye:\n",
    "\n",
    "1. **An√°lisis Exploratorio de Datos (EDA)** completo\n",
    "2. **Preprocesamiento** de datos (limpieza, escalado, balanceo)\n",
    "3. **Reducci√≥n de dimensionalidad** usando PCA\n",
    "4. **Entrenamiento y comparaci√≥n** de m√∫ltiples modelos de Machine Learning\n",
    "5. **Calibraci√≥n de probabilidades** para mejorar la confianza de las predicciones\n",
    "6. **Interpretabilidad** del modelo para entender qu√© caracter√≠sticas son m√°s importantes\n",
    "7. **An√°lisis de robustez** para evaluar la generalizaci√≥n del modelo\n",
    "8. **Conclusiones y recomendaciones** para posible uso en producci√≥n\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Configuraci√≥n del Entorno\n",
    "\n",
    "En esta secci√≥n importamos todas las librer√≠as necesarias y verificamos las versiones de las principales herramientas que vamos a utilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librer√≠as b√°sicas para manipulaci√≥n de datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Librer√≠as para visualizaci√≥n\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Librer√≠as de scikit-learn para preprocesamiento\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
    "\n",
    "# Librer√≠as para reducci√≥n de dimensionalidad\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Modelos de clasificaci√≥n\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Calibraci√≥n de modelos\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "\n",
    "# M√©tricas de evaluaci√≥n\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    balanced_accuracy_score,\n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    brier_score_loss,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# Configurar semilla aleatoria para reproducibilidad\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Mostrar versiones de las librer√≠as principales\n",
    "print(\"=\"*60)\n",
    "print(\"VERSIONES DE LIBRER√çAS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"Matplotlib: {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn: {sns.__version__}\")\n",
    "import sklearn\n",
    "print(f\"Scikit-learn: {sklearn.__version__}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Carga de Datos y An√°lisis Exploratorio (EDA)\n",
    "\n",
    "En esta secci√≥n vamos a:\n",
    "- Cargar el dataset desde el archivo CSV\n",
    "- Explorar la estructura de los datos\n",
    "- Analizar la distribuci√≥n de las variables\n",
    "- Identificar valores faltantes\n",
    "- Visualizar correlaciones entre variables\n",
    "- Detectar caracter√≠sticas con poca varianza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Carga del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset desde el archivo CSV\n",
    "# Ruta del archivo: ajustar si es necesario\n",
    "ruta_dataset = r\"Data\\archive\\star_classification.csv\"\n",
    "\n",
    "# Leer el archivo CSV\n",
    "df = pd.read_csv(ruta_dataset)\n",
    "\n",
    "print(\"Dataset cargado exitosamente.\")\n",
    "print(f\"Dimensiones del dataset: {df.shape[0]} filas y {df.shape[1]} columnas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Exploraci√≥n Inicial del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar las primeras filas del dataset para entender su estructura\n",
    "print(\"Primeras 5 filas del dataset:\")\n",
    "print(\"=\"*80)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n general del dataset: tipos de datos, valores no nulos\n",
    "print(\"Informaci√≥n general del dataset:\")\n",
    "print(\"=\"*80)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar valores faltantes por columna\n",
    "print(\"Valores faltantes por columna:\")\n",
    "print(\"=\"*80)\n",
    "valores_faltantes = df.isnull().sum()\n",
    "porcentaje_faltantes = (valores_faltantes / len(df)) * 100\n",
    "\n",
    "# Crear un DataFrame para mostrar la informaci√≥n de manera clara\n",
    "df_faltantes = pd.DataFrame({\n",
    "    'Columna': valores_faltantes.index,\n",
    "    'Valores Faltantes': valores_faltantes.values,\n",
    "    'Porcentaje (%)': porcentaje_faltantes.values\n",
    "})\n",
    "\n",
    "# Filtrar solo las columnas con valores faltantes\n",
    "df_faltantes = df_faltantes[df_faltantes['Valores Faltantes'] > 0]\n",
    "\n",
    "if len(df_faltantes) > 0:\n",
    "    print(df_faltantes.to_string(index=False))\n",
    "else:\n",
    "    print(\"No hay valores faltantes en el dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 An√°lisis de la Variable Objetivo\n",
    "\n",
    "La variable objetivo es `class`, que indica el tipo de objeto astron√≥mico:\n",
    "- **GALAXY**: Galaxia\n",
    "- **STAR**: Estrella\n",
    "- **QSO**: Cu√°sar (Quasi-Stellar Object)\n",
    "\n",
    "Vamos a analizar la distribuci√≥n de estas clases para identificar posibles desbalances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar la frecuencia de cada clase\n",
    "print(\"Distribuci√≥n de la variable objetivo 'class':\")\n",
    "print(\"=\"*80)\n",
    "conteo_clases = df['class'].value_counts()\n",
    "porcentaje_clases = (conteo_clases / len(df)) * 100\n",
    "\n",
    "# Crear DataFrame para mostrar la distribuci√≥n\n",
    "df_distribucion = pd.DataFrame({\n",
    "    'Clase': conteo_clases.index,\n",
    "    'Frecuencia': conteo_clases.values,\n",
    "    'Porcentaje (%)': porcentaje_clases.values\n",
    "})\n",
    "\n",
    "print(df_distribucion.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar la distribuci√≥n de clases con un gr√°fico de barras\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.countplot(data=df, x='class', order=df['class'].value_counts().index)\n",
    "plt.title('Distribuci√≥n de Clases en el Dataset', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Clase', fontsize=12)\n",
    "plt.ylabel('Frecuencia', fontsize=12)\n",
    "\n",
    "# A√±adir valores sobre las barras\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x() + p.get_width()/2., height,\n",
    "            f'{int(height)}\\n({height/len(df)*100:.1f}%)',\n",
    "            ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Comentario sobre el desbalance\n",
    "print(\"\\nüìä Observaci√≥n:\")\n",
    "if df_distribucion['Porcentaje (%)'].max() > 50:\n",
    "    print(\"El dataset presenta un desbalance de clases. La clase mayoritaria representa\")\n",
    "    print(f\"m√°s del 50% de las observaciones. Esto puede afectar el desempe√±o del modelo.\")\n",
    "    print(\"Consideraremos t√©cnicas de balanceo o ajuste de pesos de clase durante el entrenamiento.\")\n",
    "else:\n",
    "    print(\"El dataset tiene una distribuci√≥n relativamente balanceada entre clases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Estad√≠sticas Descriptivas de Variables Num√©ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar estad√≠sticas descriptivas de las variables num√©ricas\n",
    "print(\"Estad√≠sticas descriptivas de variables num√©ricas:\")\n",
    "print(\"=\"*80)\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar columnas num√©ricas relevantes para el an√°lisis\n",
    "# Excluiremos IDs y metadatos t√©cnicos que no aportan informaci√≥n predictiva\n",
    "columnas_a_excluir = ['obj_ID', 'run_ID', 'rerun_ID', 'cam_col', 'field_ID', 'spec_obj_ID', 'plate', 'MJD', 'fiber_ID']\n",
    "\n",
    "# Obtener todas las columnas num√©ricas excepto las excluidas\n",
    "columnas_numericas = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "columnas_caracteristicas = [col for col in columnas_numericas if col not in columnas_a_excluir]\n",
    "\n",
    "print(f\"\\nColumnas num√©ricas que se usar√°n como caracter√≠sticas: {columnas_caracteristicas}\")\n",
    "print(f\"Total de caracter√≠sticas num√©ricas: {len(columnas_caracteristicas)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 An√°lisis de Correlaci√≥n\n",
    "\n",
    "Vamos a calcular y visualizar la matriz de correlaci√≥n entre las variables num√©ricas para identificar:\n",
    "- Variables altamente correlacionadas (redundancia)\n",
    "- Patrones de relaci√≥n entre caracter√≠sticas\n",
    "- Posibles problemas de multicolinealidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la matriz de correlaci√≥n\n",
    "matriz_correlacion = df[columnas_caracteristicas].corr()\n",
    "\n",
    "# Visualizar la matriz de correlaci√≥n con un mapa de calor\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(matriz_correlacion, \n",
    "            annot=True, \n",
    "            fmt='.2f', \n",
    "            cmap='coolwarm', \n",
    "            center=0,\n",
    "            square=True,\n",
    "            linewidths=0.5,\n",
    "            cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Matriz de Correlaci√≥n de Caracter√≠sticas Num√©ricas', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Observaci√≥n:\")\n",
    "print(\"Las variables de magnitud (u, g, r, i, z) pueden estar correlacionadas entre s√≠,\")\n",
    "print(\"lo cual es esperado en datos astron√≥micos. Esto justifica el uso de PCA para\")\n",
    "print(\"reducir dimensionalidad y eliminar redundancia.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Detecci√≥n de Caracter√≠sticas con Varianza Baja\n",
    "\n",
    "Las caracter√≠sticas con varianza muy baja o nula no aportan informaci√≥n √∫til para la clasificaci√≥n. Vamos a identificarlas y decidir si eliminarlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la varianza de cada caracter√≠stica num√©rica\n",
    "varianzas = df[columnas_caracteristicas].var()\n",
    "\n",
    "print(\"Varianza de cada caracter√≠stica:\")\n",
    "print(\"=\"*80)\n",
    "print(varianzas.sort_values())\n",
    "\n",
    "# Definir un umbral para varianza baja (por ejemplo, 0.01)\n",
    "umbral_varianza = 0.01\n",
    "\n",
    "# Identificar caracter√≠sticas con varianza menor al umbral\n",
    "caracteristicas_baja_varianza = varianzas[varianzas < umbral_varianza]\n",
    "\n",
    "print(f\"\\n\\nCaracter√≠sticas con varianza menor a {umbral_varianza}:\")\n",
    "if len(caracteristicas_baja_varianza) > 0:\n",
    "    print(caracteristicas_baja_varianza)\n",
    "    print(f\"\\nSe recomienda eliminar estas {len(caracteristicas_baja_varianza)} caracter√≠stica(s).\")\n",
    "else:\n",
    "    print(\"No se encontraron caracter√≠sticas con varianza extremadamente baja.\")\n",
    "    print(\"Todas las caracter√≠sticas tienen varianza suficiente para ser consideradas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Visualizaci√≥n de Distribuciones por Clase\n",
    "\n",
    "Vamos a visualizar c√≥mo se distribuyen algunas de las caracter√≠sticas principales seg√∫n la clase del objeto astron√≥mico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar algunas caracter√≠sticas clave para visualizar\n",
    "caracteristicas_visualizar = ['u', 'g', 'r', 'i', 'z', 'redshift']\n",
    "\n",
    "# Crear subplots para visualizar distribuciones\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, caracteristica in enumerate(caracteristicas_visualizar):\n",
    "    for clase in df['class'].unique():\n",
    "        datos_clase = df[df['class'] == clase][caracteristica]\n",
    "        axes[idx].hist(datos_clase, alpha=0.6, label=clase, bins=30)\n",
    "    \n",
    "    axes[idx].set_xlabel(caracteristica, fontsize=10)\n",
    "    axes[idx].set_ylabel('Frecuencia', fontsize=10)\n",
    "    axes[idx].set_title(f'Distribuci√≥n de {caracteristica} por Clase', fontsize=11, fontweight='bold')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Observaci√≥n:\")\n",
    "print(\"Las diferentes clases muestran distribuciones distintas en las caracter√≠sticas,\")\n",
    "print(\"lo que sugiere que estas variables tienen poder discriminativo para la clasificaci√≥n.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Resumen del EDA\n",
    "\n",
    "**Conclusiones del An√°lisis Exploratorio:**\n",
    "\n",
    "1. **Tama√±o del Dataset**: El dataset contiene informaci√≥n de m√∫ltiples objetos astron√≥micos con caracter√≠sticas num√©ricas.\n",
    "\n",
    "2. **Variable Objetivo**: Tenemos 3 clases (GALAXY, STAR, QSO). La distribuci√≥n puede estar desbalanceada, lo que consideraremos en el preprocesamiento.\n",
    "\n",
    "3. **Caracter√≠sticas**: Las principales caracter√≠sticas son las magnitudes en diferentes bandas (u, g, r, i, z), coordenadas (alpha, delta) y redshift.\n",
    "\n",
    "4. **Correlaciones**: Existe correlaci√≥n entre algunas variables, especialmente entre las magnitudes, lo que justifica el uso de PCA.\n",
    "\n",
    "5. **Valores Faltantes**: Se identificaron (o no) valores faltantes que deber√°n ser tratados en el preprocesamiento.\n",
    "\n",
    "6. **Separabilidad**: Las distribuciones de caracter√≠sticas por clase sugieren que es posible discriminar entre las diferentes categor√≠as.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocesamiento de Datos\n",
    "\n",
    "En esta secci√≥n vamos a:\n",
    "- Separar caracter√≠sticas (X) y variable objetivo (y)\n",
    "- Eliminar columnas irrelevantes (IDs, metadatos)\n",
    "- Tratar valores faltantes mediante imputaci√≥n\n",
    "- Codificar la variable objetivo\n",
    "- Analizar el desbalance de clases\n",
    "- Escalar las caracter√≠sticas num√©ricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Separaci√≥n de Caracter√≠sticas y Variable Objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una copia del dataset original para no modificarlo\n",
    "df_procesado = df.copy()\n",
    "\n",
    "# Separar la variable objetivo (y) del resto de caracter√≠sticas\n",
    "y = df_procesado['class']\n",
    "\n",
    "# Seleccionar solo las caracter√≠sticas num√©ricas relevantes (X)\n",
    "# Eliminamos IDs y metadatos que no aportan informaci√≥n predictiva\n",
    "X = df_procesado[columnas_caracteristicas]\n",
    "\n",
    "print(f\"Dimensiones de X (caracter√≠sticas): {X.shape}\")\n",
    "print(f\"Dimensiones de y (variable objetivo): {y.shape}\")\n",
    "print(f\"\\nCaracter√≠sticas seleccionadas: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tratamiento de Valores Faltantes\n",
    "\n",
    "**Estrategia de imputaci√≥n:**\n",
    "- Para caracter√≠sticas num√©ricas, utilizaremos `SimpleImputer` con la **mediana**\n",
    "- Elegimos la mediana porque es m√°s robusta a valores at√≠picos (outliers) que la media\n",
    "- Esto es importante en datos astron√≥micos que pueden tener mediciones extremas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si hay valores faltantes en X\n",
    "print(\"Valores faltantes en X antes de la imputaci√≥n:\")\n",
    "print(X.isnull().sum())\n",
    "\n",
    "# Crear el imputador con estrategia de mediana\n",
    "imputador = SimpleImputer(strategy='median')\n",
    "\n",
    "# Ajustar y transformar los datos\n",
    "X_imputado = imputador.fit_transform(X)\n",
    "\n",
    "# Convertir de nuevo a DataFrame para mantener los nombres de las columnas\n",
    "X_imputado = pd.DataFrame(X_imputado, columns=X.columns, index=X.index)\n",
    "\n",
    "print(\"\\nValores faltantes en X despu√©s de la imputaci√≥n:\")\n",
    "print(X_imputado.isnull().sum())\n",
    "print(\"\\n‚úì Imputaci√≥n completada exitosamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Codificaci√≥n de la Variable Objetivo\n",
    "\n",
    "Necesitamos convertir las etiquetas de texto (GALAXY, STAR, QSO) a valores num√©ricos para poder entrenar los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el codificador de etiquetas\n",
    "codificador_etiquetas = LabelEncoder()\n",
    "\n",
    "# Ajustar y transformar las etiquetas\n",
    "y_codificado = codificador_etiquetas.fit_transform(y)\n",
    "\n",
    "# Mostrar la correspondencia entre etiquetas originales y codificadas\n",
    "print(\"Correspondencia de etiquetas:\")\n",
    "print(\"=\"*40)\n",
    "for i, clase in enumerate(codificador_etiquetas.classes_):\n",
    "    print(f\"{clase} ‚Üí {i}\")\n",
    "\n",
    "# Guardar las clases originales para uso posterior\n",
    "clases_originales = codificador_etiquetas.classes_\n",
    "print(f\"\\n‚úì Codificaci√≥n completada. Total de clases: {len(clases_originales)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 An√°lisis de Desbalance de Clases\n",
    "\n",
    "El desbalance de clases puede causar que el modelo favorezca la clase mayoritaria. Vamos a calcular la distribuci√≥n y considerar estrategias de balanceo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la distribuci√≥n de clases en t√©rminos de frecuencia\n",
    "unique, counts = np.unique(y_codificado, return_counts=True)\n",
    "distribucion_clases = dict(zip(unique, counts))\n",
    "\n",
    "print(\"Distribuci√≥n de clases (codificadas):\")\n",
    "print(\"=\"*40)\n",
    "for clase_num, conteo in distribucion_clases.items():\n",
    "    clase_nombre = clases_originales[clase_num]\n",
    "    porcentaje = (conteo / len(y_codificado)) * 100\n",
    "    print(f\"Clase {clase_num} ({clase_nombre}): {conteo} ({porcentaje:.2f}%)\")\n",
    "\n",
    "# Calcular el ratio de desbalance\n",
    "max_clase = max(counts)\n",
    "min_clase = min(counts)\n",
    "ratio_desbalance = max_clase / min_clase\n",
    "\n",
    "print(f\"\\nRatio de desbalance (max/min): {ratio_desbalance:.2f}\")\n",
    "\n",
    "# Determinar estrategia de balanceo\n",
    "if ratio_desbalance > 3:\n",
    "    print(\"\\n‚ö†Ô∏è El dataset presenta un desbalance significativo.\")\n",
    "    print(\"Estrategia recomendada:\")\n",
    "    print(\"  1. Usar 'class_weight=balanced' en los modelos que lo soporten\")\n",
    "    print(\"  2. Considerar t√©cnicas de sobremuestreo (SMOTE) o submuestreo\")\n",
    "    print(\"  3. Usar m√©tricas como F1-score macro y Balanced Accuracy\")\n",
    "    usar_class_weight = True\n",
    "else:\n",
    "    print(\"\\n‚úì El dataset tiene un desbalance moderado o est√° balanceado.\")\n",
    "    print(\"No es cr√≠tico usar t√©cnicas de balanceo, pero consideraremos class_weight.\")\n",
    "    usar_class_weight = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Escalado de Caracter√≠sticas\n",
    "\n",
    "**¬øPor qu√© escalar?**\n",
    "- Muchos algoritmos (SVM, KNN, redes neuronales, PCA) son sensibles a la escala de las caracter√≠sticas\n",
    "- El escalado asegura que todas las variables tengan la misma importancia inicial\n",
    "\n",
    "**Elecci√≥n del escalador:**\n",
    "- **StandardScaler**: Estandariza las caracter√≠sticas para tener media 0 y desviaci√≥n est√°ndar 1\n",
    "- **RobustScaler**: Alternativa m√°s robusta a outliers (usa mediana y rango intercuartil)\n",
    "\n",
    "Usaremos `StandardScaler` por defecto, pero comentaremos cu√°ndo usar `RobustScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el escalador est√°ndar\n",
    "escalador = StandardScaler()\n",
    "\n",
    "# Ajustar el escalador con los datos y transformar\n",
    "X_escalado = escalador.fit_transform(X_imputado)\n",
    "\n",
    "# Convertir de nuevo a DataFrame para mantener los nombres de columnas\n",
    "X_escalado = pd.DataFrame(X_escalado, columns=X.columns, index=X.index)\n",
    "\n",
    "print(\"‚úì Escalado completado exitosamente.\")\n",
    "print(\"\\nEstad√≠sticas despu√©s del escalado:\")\n",
    "print(\"=\"*60)\n",
    "print(\"Medias (deben estar cerca de 0):\")\n",
    "print(X_escalado.mean())\n",
    "print(\"\\nDesviaciones est√°ndar (deben estar cerca de 1):\")\n",
    "print(X_escalado.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Resumen del Preprocesamiento\n",
    "\n",
    "**Pasos completados:**\n",
    "\n",
    "1. ‚úì Separaci√≥n de caracter√≠sticas (X) y variable objetivo (y)\n",
    "2. ‚úì Eliminaci√≥n de columnas irrelevantes (IDs y metadatos)\n",
    "3. ‚úì Imputaci√≥n de valores faltantes usando la mediana\n",
    "4. ‚úì Codificaci√≥n de la variable objetivo (texto ‚Üí n√∫meros)\n",
    "5. ‚úì An√°lisis de desbalance de clases y definici√≥n de estrategia\n",
    "6. ‚úì Escalado de caracter√≠sticas usando StandardScaler\n",
    "\n",
    "**Datos listos para:**\n",
    "- Reducci√≥n de dimensionalidad (PCA)\n",
    "- Divisi√≥n en conjuntos de entrenamiento y prueba\n",
    "- Entrenamiento de modelos\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reducci√≥n de Dimensionalidad con PCA\n",
    "\n",
    "El An√°lisis de Componentes Principales (PCA) nos permite:\n",
    "- Reducir la dimensionalidad del dataset\n",
    "- Eliminar redundancia entre caracter√≠sticas correlacionadas\n",
    "- Mejorar la eficiencia computacional\n",
    "- Facilitar la visualizaci√≥n de los datos\n",
    "- Potencialmente mejorar el desempe√±o de algunos modelos\n",
    "\n",
    "**Objetivos:**\n",
    "1. Aplicar PCA sobre los datos escalados\n",
    "2. Analizar la varianza explicada por componente\n",
    "3. Elegir el n√∫mero √≥ptimo de componentes\n",
    "4. Visualizar los datos en 2D usando las primeras componentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Aplicaci√≥n de PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero, aplicamos PCA con todas las componentes posibles para analizar la varianza\n",
    "pca_completo = PCA(random_state=SEED)\n",
    "pca_completo.fit(X_escalado)\n",
    "\n",
    "# Obtener la varianza explicada por cada componente\n",
    "varianza_explicada = pca_completo.explained_variance_ratio_\n",
    "varianza_acumulada = np.cumsum(varianza_explicada)\n",
    "\n",
    "print(\"Varianza explicada por cada componente principal:\")\n",
    "print(\"=\"*60)\n",
    "for i, (var_individual, var_acum) in enumerate(zip(varianza_explicada, varianza_acumulada)):\n",
    "    print(f\"PC{i+1}: {var_individual*100:.2f}% | Acumulada: {var_acum*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualizaci√≥n de la Varianza Explicada (Gr√°fico del Codo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear figura con dos subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Subplot 1: Varianza explicada por componente individual\n",
    "ax1.bar(range(1, len(varianza_explicada) + 1), varianza_explicada * 100)\n",
    "ax1.set_xlabel('Componente Principal', fontsize=12)\n",
    "ax1.set_ylabel('Varianza Explicada (%)', fontsize=12)\n",
    "ax1.set_title('Varianza Explicada por Componente Individual', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Varianza explicada acumulada\n",
    "ax2.plot(range(1, len(varianza_acumulada) + 1), varianza_acumulada * 100, 'o-', linewidth=2, markersize=8)\n",
    "ax2.axhline(y=90, color='r', linestyle='--', label='90% varianza')\n",
    "ax2.axhline(y=95, color='g', linestyle='--', label='95% varianza')\n",
    "ax2.set_xlabel('N√∫mero de Componentes', fontsize=12)\n",
    "ax2.set_ylabel('Varianza Explicada Acumulada (%)', fontsize=12)\n",
    "ax2.set_title('Varianza Explicada Acumulada (Gr√°fico del Codo)', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Selecci√≥n del N√∫mero √ìptimo de Componentes\n",
    "\n",
    "**Criterio:** Seleccionamos el n√∫mero m√≠nimo de componentes que expliquen al menos el 95% de la varianza total.\n",
    "\n",
    "**Justificaci√≥n:** \n",
    "- Mantener el 95% de la varianza asegura que conservamos la mayor parte de la informaci√≥n\n",
    "- Al mismo tiempo, reducimos significativamente la dimensionalidad\n",
    "- Esto mejora la eficiencia computacional sin sacrificar mucha informaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determinar el n√∫mero de componentes necesarios para explicar el 95% de la varianza\n",
    "umbral_varianza = 0.95\n",
    "n_componentes_95 = np.argmax(varianza_acumulada >= umbral_varianza) + 1\n",
    "\n",
    "print(f\"N√∫mero de componentes para {umbral_varianza*100}% de varianza: {n_componentes_95}\")\n",
    "print(f\"Varianza explicada con {n_componentes_95} componentes: {varianza_acumulada[n_componentes_95-1]*100:.2f}%\")\n",
    "\n",
    "# Tambi√©n mostrar para 90% y 99% como referencia\n",
    "n_componentes_90 = np.argmax(varianza_acumulada >= 0.90) + 1\n",
    "n_componentes_99 = np.argmax(varianza_acumulada >= 0.99) + 1\n",
    "\n",
    "print(f\"\\nComparaci√≥n:\")\n",
    "print(f\"  - 90% varianza: {n_componentes_90} componentes\")\n",
    "print(f\"  - 95% varianza: {n_componentes_95} componentes\")\n",
    "print(f\"  - 99% varianza: {n_componentes_99} componentes\")\n",
    "print(f\"\\nDimensionalidad original: {X_escalado.shape[1]} caracter√≠sticas\")\n",
    "print(f\"Dimensionalidad reducida: {n_componentes_95} componentes\")\n",
    "print(f\"Reducci√≥n: {(1 - n_componentes_95/X_escalado.shape[1])*100:.1f}%\")\n",
    "\n",
    "# Usar el n√∫mero de componentes para 95% de varianza\n",
    "n_componentes_seleccionado = n_componentes_95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Transformaci√≥n de Datos con PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar PCA con el n√∫mero de componentes seleccionado\n",
    "pca = PCA(n_components=n_componentes_seleccionado, random_state=SEED)\n",
    "X_pca = pca.fit_transform(X_escalado)\n",
    "\n",
    "# Convertir a DataFrame para facilitar el manejo\n",
    "columnas_pca = [f'PC{i+1}' for i in range(n_componentes_seleccionado)]\n",
    "X_pca_df = pd.DataFrame(X_pca, columns=columnas_pca, index=X_escalado.index)\n",
    "\n",
    "print(f\"‚úì PCA aplicado exitosamente.\")\n",
    "print(f\"\\nDimensiones de X despu√©s de PCA: {X_pca_df.shape}\")\n",
    "print(f\"Varianza total explicada: {pca.explained_variance_ratio_.sum()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Visualizaci√≥n de Datos en 2D usando PCA\n",
    "\n",
    "Vamos a visualizar los datos proyectados en las dos primeras componentes principales, coloreados por clase. Esto nos ayuda a:\n",
    "- Entender la separabilidad de las clases en el espacio reducido\n",
    "- Identificar posibles solapamientos entre clases\n",
    "- Validar visualmente que PCA mantiene la estructura de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el gr√°fico de dispersi√≥n 2D\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Graficar cada clase con un color diferente\n",
    "for i, clase in enumerate(clases_originales):\n",
    "    # Filtrar puntos de esta clase\n",
    "    mascara = (y_codificado == i)\n",
    "    plt.scatter(X_pca_df.loc[mascara, 'PC1'], \n",
    "               X_pca_df.loc[mascara, 'PC2'],\n",
    "               label=clase,\n",
    "               alpha=0.6,\n",
    "               s=20)\n",
    "\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% varianza)', fontsize=12)\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% varianza)', fontsize=12)\n",
    "plt.title('Proyecci√≥n de Datos en las Primeras Dos Componentes Principales', fontsize=14, fontweight='bold')\n",
    "plt.legend(title='Clase', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Observaci√≥n:\")\n",
    "print(\"En el gr√°fico podemos observar c√≥mo se distribuyen las diferentes clases en el espacio\")\n",
    "print(\"de las dos primeras componentes principales. Una buena separaci√≥n visual indica que\")\n",
    "print(\"los modelos de clasificaci√≥n deber√≠an poder distinguir entre las clases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 An√°lisis de Componentes Principales (Opcional)\n",
    "\n",
    "Podemos analizar qu√© caracter√≠sticas originales contribuyen m√°s a cada componente principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrame con los componentes (loadings) de PCA\n",
    "componentes_df = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=columnas_pca,\n",
    "    index=X_escalado.columns\n",
    ")\n",
    "\n",
    "print(\"Contribuci√≥n de cada caracter√≠stica original a las componentes principales:\")\n",
    "print(\"=\"*80)\n",
    "print(componentes_df.round(3))\n",
    "\n",
    "# Visualizar las contribuciones de las primeras 3 componentes\n",
    "fig, axes = plt.subplots(1, min(3, n_componentes_seleccionado), figsize=(18, 5))\n",
    "if n_componentes_seleccionado == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i in range(min(3, n_componentes_seleccionado)):\n",
    "    componentes_df[f'PC{i+1}'].plot(kind='barh', ax=axes[i])\n",
    "    axes[i].set_title(f'Contribuciones a PC{i+1}\\n({pca.explained_variance_ratio_[i]*100:.1f}% varianza)', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "    axes[i].set_xlabel('Peso', fontsize=10)\n",
    "    axes[i].axvline(x=0, color='k', linestyle='-', linewidth=0.8)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Resumen de Reducci√≥n de Dimensionalidad\n",
    "\n",
    "**Logros de PCA:**\n",
    "\n",
    "1. ‚úì Reducci√≥n significativa de dimensionalidad manteniendo ~95% de la varianza\n",
    "2. ‚úì Eliminaci√≥n de redundancia entre caracter√≠sticas correlacionadas\n",
    "3. ‚úì Visualizaci√≥n exitosa de los datos en 2D\n",
    "4. ‚úì Identificaci√≥n de las caracter√≠sticas m√°s importantes para cada componente\n",
    "\n",
    "**Datos disponibles:**\n",
    "- `X_escalado`: Datos originales escalados (todas las caracter√≠sticas)\n",
    "- `X_pca_df`: Datos transformados con PCA (dimensionalidad reducida)\n",
    "\n",
    "**Decisi√≥n:** Usaremos `X_pca_df` para entrenar los modelos, ya que esto mejorar√° la eficiencia computacional y podr√≠a mejorar el desempe√±o al reducir el ruido.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
